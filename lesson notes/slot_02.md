# **Slot 2: Google Colab & Building Computer Vision Models**

**NgÃ y há»c:** 08-01-2026

**MÃ´n há»c:** AI Development with TensorFlow (DAT301m)

**TÃ i liá»‡u tham kháº£o:**
- 1.3 Coding with TensorFlow in Google Colaboratory.pptx
- 1.4 Computer Vision Neural Network.pptx

---

## I. MÃ´i trÆ°á»ng thá»±c hÃ nh: Google Colab

### 1. Google Colab lÃ  gÃ¬?
* LÃ  mÃ´i trÆ°á»ng Jupyter Notebook Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn cloud (hosted) bá»Ÿi Google.
* **Æ¯u Ä‘iá»ƒm:**
    * KhÃ´ng cáº§n cÃ i Ä‘áº·t (No setup required).
    * Miá»…n phÃ­ truy cáº­p GPU/TPU (Ráº¥t quan trá»ng Ä‘á»ƒ train model nhanh).
    * TensorFlow Ä‘Ã£ Ä‘Æ°á»£c cÃ i sáºµn.

### 2. CÃ¡c bÆ°á»›c thiáº¿t láº­p cÆ¡ báº£n
1.  Truy cáº­p: `colab.research.google.com`
2.  ÄÄƒng nháº­p Google Account -> Create New Notebook.
3.  **Báº­t GPU:** VÃ o menu *Runtime* -> *Change runtime type* -> Chá»n *GPU* (hoáº·c TPU).

### 3. Lá»—i phá»• biáº¿n 
- CÃ¡ch kháº¯c phá»¥c khi cháº¡y Tensorflow mÃ  khÃ´ng dÃ¹ng Ä‘Æ°á»£c GPU Ä‘á»ƒ cháº¡y model. (ThÆ°á»ng sáº½ gáº·p trÃªn cÃ¡c mÃ¡y Window)

[Installing TensorFlow 2 GPU [Step-by-Step Guide]d](https://neptune.ai/blog/installing-tensorflow-2-gpu-guide)

---

## II. XÃ¢y dá»±ng Máº¡ng NÆ¡-ron cho Computer Vision

Quy trÃ¬nh xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh Deep Learning chuáº©n gá»“m 4 bÆ°á»›c chÃ­nh:

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u (Data Loading & Preprocessing)
* **Dataset:** Fashion-MNIST (Bá»™ dá»¯ liá»‡u quáº§n Ã¡o cá»§a Zalando).
    * Sá»‘ lÆ°á»£ng: 60,000 áº£nh Train, 10,000 áº£nh Test.
    * KÃ­ch thÆ°á»›c: 28x28 pixel (Grayscale - áº£nh xÃ¡m).
    * NhÃ£n (Labels): 10 loáº¡i (0: Ão thun, 1: Quáº§n, 9: GiÃ y boot...).
* **Normalization (Chuáº©n hÃ³a):** Chia giÃ¡ trá»‹ pixel cho 255.
    ```python
    training_images = training_images / 255.0
    test_images = test_images / 255.0
    ```
    > **Giáº£i thÃ­ch thuáº­t ngá»¯:**
    > * **Táº¡i sao chia cho 255?**
    >     * *Minh há»a:* Pixel áº£nh cÃ³ giÃ¡ trá»‹ tá»« 0 (Ä‘en) Ä‘áº¿n 255 (tráº¯ng). MÃ¡y tÃ­nh xá»­ lÃ½ sá»‘ nhá» (tá»« 0 Ä‘áº¿n 1) nhanh vÃ  á»•n Ä‘á»‹nh hÆ¡n sá»‘ lá»›n. Viá»‡c nÃ y giá»‘ng nhÆ° viá»‡c quy Ä‘á»•i tiá»n tá»‡ tá»« VNÄ sang USD Ä‘á»ƒ con sá»‘ nhá» gá»n hÆ¡n, dá»… tÃ­nh toÃ¡n hÆ¡n.

### BÆ°á»›c 2: Äá»‹nh nghÄ©a MÃ´ hÃ¬nh (Model Definition)

Sá»­ dá»¥ng `tf.keras.models.Sequential` (MÃ´ hÃ¬nh tuáº§n tá»± - cÃ¡c lá»›p xáº¿p chá»“ng lÃªn nhau).

```python
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])

```

> **Giáº£i thÃ­ch thuáº­t ngá»¯ & Minh há»a:**
> 1. **Flatten (LÃ m pháº³ng):**
> * *Chá»©c nÄƒng:* Biáº¿n Ä‘á»•i áº£nh 2D (hÃ¬nh vuÃ´ng 28x28) thÃ nh máº£ng 1D (má»™t hÃ ng dá»c 784 Ä‘iá»ƒm).
> * *Minh há»a:* Giá»‘ng nhÆ° báº¡n thÃ¡o má»™t chiáº¿c há»™p giáº¥y vuÃ´ng ra vÃ  tráº£i pháº³ng nÃ³ lÃªn máº·t bÃ n Ä‘á»ƒ dá»… quan sÃ¡t táº¥t cáº£ cÃ¡c máº·t cÃ¹ng lÃºc.
> 
> 2. **Dense (Lá»›p dÃ y Ä‘áº·c):**
> * *Chá»©c nÄƒng:* Lá»›p nÆ¡-ron thÃ´ng thÆ°á»ng, nÆ¡i má»i nÆ¡-ron káº¿t ná»‘i vá»›i táº¥t cáº£ nÆ¡-ron lá»›p trÆ°á»›c.
> * *Minh há»a:* Giá»‘ng nhÆ° má»™t cuá»™c há»p mÃ  **táº¥t cáº£** má»i ngÆ°á»i Ä‘á»u báº¯t tay nhau. 128 nÆ¡-ron lÃ  128 "chuyÃªn gia" Ä‘ang cá»‘ gáº¯ng tÃ¬m ra cÃ¡c Ä‘áº·c Ä‘iá»ƒm cá»§a bá»©c áº£nh.
> 
> 3. **Relu (HÃ m kÃ­ch hoáº¡t):**
> * *Quy táº¯c:* `Náº¿u x > 0 thÃ¬ giá»¯ nguyÃªn, náº¿u x < 0 thÃ¬ báº±ng 0`.
> * *Minh há»a:* Giá»‘ng nhÆ° má»™t **bá»™ lá»c tiáº¿ng á»“n**. Nhá»¯ng tÃ­n hiá»‡u tiÃªu cá»±c (khÃ´ng quan trá»ng) bá»‹ loáº¡i bá», chá»‰ giá»¯ láº¡i tÃ­n hiá»‡u tÃ­ch cá»±c (quan trá»ng) Ä‘á»ƒ truyá»n sang lá»›p sau.
>
> 4. **Softmax:**
> * *Chá»©c nÄƒng:* DÃ¹ng á»Ÿ lá»›p cuá»‘i cÃ¹ng. Biáº¿n Ä‘á»•i cÃ¡c con sá»‘ lá»™n xá»™n thÃ nh **xÃ¡c suáº¥t (%)** sao cho tá»•ng báº±ng 100%.
> * *VÃ­ dá»¥:* Thay vÃ¬ nÃ³i "Äiá»ƒm sá»‘ lÃ  5, 2, 9", Softmax sáº½ nÃ³i "Kháº£ nÄƒng lÃ  Ão thun: 10%, Quáº§n: 5%, GiÃ y: 85%". Ta chá»n cÃ¡i cao nháº¥t.

### BÆ°á»›c 3: BiÃªn dá»‹ch & Huáº¥n luyá»‡n (Compile & Train)

```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(training_images, training_labels, epochs=5)

```

> **Giáº£i thÃ­ch thuáº­t ngá»¯:**
> * **Loss Function (HÃ m máº¥t mÃ¡t):** ThÆ°á»›c Ä‘o xem mÃ´ hÃ¬nh Ä‘oÃ¡n **SAI** bao nhiÃªu. Má»¥c tiÃªu lÃ  giáº£m Loss cÃ ng tháº¥p cÃ ng tá»‘t.
> * **Optimizer (Bá»™ tá»‘i Æ°u hÃ³a):** "NgÆ°á»i dáº«n Ä‘Æ°á»ng" dá»±a trÃªn Loss Ä‘á»ƒ Ä‘iá»u chá»‰nh láº¡i cÃ¡c tham sá»‘ (weights) sao cho láº§n Ä‘oÃ¡n sau chÃ­nh xÃ¡c hÆ¡n.
> * **Epochs:** Sá»‘ láº§n mÃ´ hÃ¬nh Ä‘Æ°á»£c há»c trá»n váº¹n bá»™ dá»¯ liá»‡u. 5 epochs nghÄ©a lÃ  nÃ³ xem Ä‘i xem láº¡i sÃ¡ch giÃ¡o khoa 5 láº§n.

---

## III. Kiá»ƒm soÃ¡t quy trÃ¬nh Train (Callbacks)

Váº¥n Ä‘á»: LÃ m sao Ä‘á»ƒ dá»«ng train khi mÃ´ hÃ¬nh Ä‘Ã£ Ä‘á»§ tá»‘t (Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian vÃ  trÃ¡nh há»c váº¹t)? -> Sá»­ dá»¥ng **Callbacks**.

CÃ¡c loáº¡i Callbacks phá»• biáº¿n:

1. **ModelCheckpoint:**
* Tá»± Ä‘á»™ng lÆ°u láº¡i mÃ´ hÃ¬nh (save game) má»—i khi nÃ³ Ä‘áº¡t káº¿t quáº£ tá»‘t nháº¥t. GiÃºp báº¡n khÃ´ng bá»‹ máº¥t cÃ´ng sá»©c náº¿u mÃ¡y tÃ­nh sáº­p nguá»“n.

2. **EarlyStopping:**
* Tá»± Ä‘á»™ng dá»«ng train náº¿u tháº¥y mÃ´ hÃ¬nh khÃ´ng cÃ²n tiáº¿n bá»™ ná»¯a (há»c mÃ£i khÃ´ng giá»i thÃªm thÃ¬ cho nghá»‰ sá»›m).

3. **TensorBoard:**
* CÃ´ng cá»¥ váº½ biá»ƒu Ä‘á»“ trá»±c quan quÃ¡ trÃ¬nh train.

### Code máº«u sá»­ dá»¥ng Callback (Dá»«ng khi loss < 0.4):

```python
class MyCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('loss') < 0.4):
      print("\nÄÃ£ Ä‘áº¡t Loss < 0.4, dá»«ng train!")
      self.model.stop_training = True

callbacks = MyCallback()
model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])

```

### CÃ¡c cÃ¡ch sá»­ dá»¥ng Callbacks nÃ¢ng cao:

#### 1. EarlyStopping - Dá»«ng sá»›m khi khÃ´ng cáº£i thiá»‡n
```python
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',      # Theo dÃµi validation loss
    patience=3,              # Chá» 3 epochs khÃ´ng cáº£i thiá»‡n
    restore_best_weights=True # KhÃ´i phá»¥c weights tá»‘t nháº¥t
)
```

#### 2. ModelCheckpoint - LÆ°u model tá»‘t nháº¥t
```python
checkpoint = tf.keras.callbacks.ModelCheckpoint(
    filepath='best_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)
```

#### 3. ReduceLROnPlateau - Giáº£m learning rate tá»± Ä‘á»™ng
```python
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,              # Giáº£m LR xuá»‘ng 50%
    patience=2,
    min_lr=1e-7,
    verbose=1
)
```

#### 4. TensorBoard - Trá»±c quan hÃ³a quÃ¡ trÃ¬nh train
```python
tensorboard = tf.keras.callbacks.TensorBoard(
    log_dir='./logs',
    histogram_freq=1,
    write_graph=True
)
```

#### 5. Káº¿t há»£p nhiá»u Callbacks
```python
model.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=50,
    callbacks=[early_stopping, checkpoint, reduce_lr, tensorboard]
)
```

---

## III.A. CÃ¡ch tÃ­nh Parameters vÃ  So sÃ¡nh vá»›i GPU

### 1. CÃ´ng thá»©c tÃ­nh Parameters (Tham sá»‘)

Má»—i lá»›p Dense cÃ³ sá»‘ parameters Ä‘Æ°á»£c tÃ­nh nhÆ° sau:
$$\text{Parameters} = (\text{Input} \times \text{Output}) + \text{Bias}$$

**VÃ­ dá»¥ vá»›i model Fashion-MNIST:**
```python
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),  # 0 params
  tf.keras.layers.Dense(128, activation='relu'),  # ?
  tf.keras.layers.Dense(10, activation='softmax') # ?
])
```

**TÃ­nh toÃ¡n chi tiáº¿t:**
1. **Flatten:** 0 parameters (chá»‰ reshape, khÃ´ng há»c gÃ¬)
2. **Dense(128):** 
   - Input: 28Ã—28 = 784 neurons
   - Output: 128 neurons
   - Parameters = (784 Ã— 128) + 128 = **100,480** parameters
3. **Dense(10):**
   - Input: 128 neurons
   - Output: 10 neurons  
   - Parameters = (128 Ã— 10) + 10 = **1,290** parameters

**Tá»•ng cá»™ng: 101,770 parameters**

### 2. Kiá»ƒm tra Parameters trong code

```python
model.summary()
```

Output:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
dense (Dense)                (None, 128)               100480    
dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
```

### 3. So sÃ¡nh vá»›i GPU - CÃ³ Ä‘á»§ sá»©c cháº¡y khÃ´ng?

#### Bá»™ nhá»› cáº§n thiáº¿t cho training:
$$\text{Memory} = \text{Parameters} \times \text{Bytes per param} \times \text{Overhead factor}$$

**VÃ­ dá»¥ tÃ­nh toÃ¡n thá»±c táº¿:**
- Model cÃ³ 101,770 params
- Má»—i param dÃ¹ng float32 = 4 bytes
- Overhead (gradients, optimizer states) â‰ˆ **4x** (Adam optimizer cáº§n lÆ°u momentum)

```
Memory needed = 101,770 Ã— 4 bytes Ã— 4 
              â‰ˆ 1.6 MB (chá»‰ cho model + gradients)
```

**Vá»›i batch size = 32:**
```
Batch memory = 32 Ã— 784 Ã— 4 bytes â‰ˆ 100 KB
Total â‰ˆ 1.7 MB
```

#### Báº£ng so sÃ¡nh GPU phá»• biáº¿n:

| GPU Model | VRAM | CÃ³ thá»ƒ train model gÃ¬? |
|-----------|------|------------------------|
| **GTX 1650** | 4GB | Models < 50M params (ResNet-18, MobileNet) |
| **RTX 3060** | 12GB | Models < 200M params (ResNet-50, EfficientNet) |
| **RTX 4090** | 24GB | Models < 500M params (BERT-large, GPT-2) |
| **A100 (Colab Pro)** | 40GB | Models < 1B params (GPT-3 small, CLIP) |

**Quy táº¯c ngÃ³n tay cÃ¡i:**
- Model nhá» (< 10M params): Cháº¡y Ä‘Æ°á»£c trÃªn CPU/GPU cÆ¡ báº£n
- Model vá»«a (10M-100M params): Cáº§n GPU cÃ³ 6GB+ VRAM
- Model lá»›n (100M-1B params): Cáº§n GPU chuyÃªn nghiá»‡p (16GB+)
- Model siÃªu lá»›n (> 1B params): Cáº§n nhiá»u GPU hoáº·c TPU

**CÃ´ng thá»©c Æ°á»›c lÆ°á»£ng nhanh:**
```python
def estimate_memory_gb(num_params, batch_size=32, bytes_per_param=4, overhead=4):
    """
    Æ¯á»›c lÆ°á»£ng VRAM cáº§n thiáº¿t
    overhead: 4 cho Adam, 2 cho SGD
    """
    model_memory = num_params * bytes_per_param * overhead
    return model_memory / (1024**3)  # Convert to GB

# VÃ­ dá»¥: ResNet-50 cÃ³ 25M params
print(f"ResNet-50 cáº§n: {estimate_memory_gb(25_000_000):.2f} GB")
# Output: ResNet-50 cáº§n: 0.37 GB (thá»±c táº¿ cáº§n 2-3GB do activation maps)
```

---

## III.B. TensorBoard - Trá»±c quan hÃ³a quÃ¡ trÃ¬nh Training

### 1. TensorBoard lÃ  gÃ¬?
TensorBoard lÃ  cÃ´ng cá»¥ trá»±c quan hÃ³a cá»§a TensorFlow, giÃºp báº¡n:
- Theo dÃµi loss vÃ  accuracy theo thá»i gian
- Xem kiáº¿n trÃºc mÃ´ hÃ¬nh (graph)
- PhÃ¢n tÃ­ch phÃ¢n phá»‘i weights
- So sÃ¡nh nhiá»u láº§n cháº¡y

### 2. Setup TensorBoard tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Import vÃ  táº¡o log directory
```python
import tensorflow as tf
import datetime
import os

# Táº¡o thÆ° má»¥c logs vá»›i timestamp
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,      # Ghi histogram má»—i epoch
    write_graph=True,      # LÆ°u computational graph
    write_images=True,     # LÆ°u áº£nh (náº¿u cÃ³)
    update_freq='epoch',   # Cáº­p nháº­t má»—i epoch
    profile_batch='500,520' # Profile batch 500-520
)
```

#### BÆ°á»›c 2: Train model vá»›i TensorBoard callback
```python
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train vá»›i TensorBoard callback
history = model.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=10,
    callbacks=[tensorboard_callback]
)
```

#### BÆ°á»›c 3: Khá»Ÿi cháº¡y TensorBoard

**Trong Jupyter Notebook/Colab:**
```python
# Load extension
%load_ext tensorboard

# Khá»Ÿi cháº¡y TensorBoard
%tensorboard --logdir logs/fit
```

**Trong Terminal:**
```bash
tensorboard --logdir logs/fit --port 6006
# Má»Ÿ browser: http://localhost:6006
```

### 3. CÃ¡c Tab quan trá»ng trong TensorBoard

#### Tab SCALARS - Theo dÃµi metrics
```python
# Log custom metrics
file_writer = tf.summary.create_file_writer(log_dir)

with file_writer.as_default():
    for epoch in range(10):
        # Giáº£ láº­p metrics
        loss = 1.0 / (epoch + 1)
        accuracy = epoch / 10.0
        
        tf.summary.scalar('custom_loss', loss, step=epoch)
        tf.summary.scalar('custom_accuracy', accuracy, step=epoch)
```

#### Tab GRAPHS - Xem kiáº¿n trÃºc model
```python
# TensorBoard tá»± Ä‘á»™ng táº¡o graph khi write_graph=True
# Báº¡n sáº½ tháº¥y visual representation cá»§a model architecture
```

#### Tab DISTRIBUTIONS - PhÃ¢n phá»‘i weights
```python
# Xem phÃ¢n phá»‘i weights/biases cá»§a tá»«ng layer qua cÃ¡c epoch
# GiÃºp phÃ¡t hiá»‡n:
# - Vanishing gradients (weights gáº§n 0)
# - Exploding gradients (weights quÃ¡ lá»›n)
```

#### Tab HISTOGRAMS - Chi tiáº¿t hÆ¡n distributions
```python
# Hiá»ƒn thá»‹ histogram 3D cá»§a weights theo thá»i gian
```

### 4. So sÃ¡nh nhiá»u láº§n cháº¡y

```python
# Run 1: Learning rate = 0.001
log_dir_1 = "logs/fit/run1_lr0.001"
tensorboard_callback_1 = tf.keras.callbacks.TensorBoard(log_dir=log_dir_1)

# Run 2: Learning rate = 0.01
log_dir_2 = "logs/fit/run2_lr0.01"
tensorboard_callback_2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir_2)

# Trong TensorBoard, cáº£ 2 runs sáº½ hiá»ƒn thá»‹ trÃªn cÃ¹ng 1 Ä‘á»“ thá»‹
%tensorboard --logdir logs/fit
```

### 5. Tips sá»­ dá»¥ng TensorBoard hiá»‡u quáº£

```python
# Tip 1: Sá»­ dá»¥ng name prefix rÃµ rÃ ng
log_dir = f"logs/{model_name}/lr_{learning_rate}/batch_{batch_size}/{timestamp}"

# Tip 2: Log learning rate
lr_callback = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-3 * 0.9 ** epoch
)

# Tip 3: Custom callback Ä‘á»ƒ log thÃªm metrics
class CustomTensorBoard(tf.keras.callbacks.Callback):
    def __init__(self, log_dir):
        super().__init__()
        self.file_writer = tf.summary.create_file_writer(log_dir)
    
    def on_epoch_end(self, epoch, logs=None):
        with self.file_writer.as_default():
            # Log custom metrics
            tf.summary.scalar('learning_rate', 
                            self.model.optimizer.lr.numpy(), 
                            step=epoch)
```

---

## III.C. TensorFlow Graph (tf.Graph)

### 1. tf.Graph lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:** 
tf.Graph lÃ  má»™t cáº¥u trÃºc dá»¯ liá»‡u biá»ƒu diá»…n cÃ¡c phÃ©p tÃ­nh (operations) dÆ°á»›i dáº¡ng **Ä‘á»“ thá»‹ cÃ³ hÆ°á»›ng** (directed graph).

**Minh há»a:**
```
Input (28x28) 
    â†“
[Flatten: 784 neurons]
    â†“
[Dense: 128 neurons] â† Weights (784Ã—128)
    â†“
[ReLU Activation]
    â†“
[Dense: 10 neurons] â† Weights (128Ã—10)
    â†“
[Softmax]
    â†“
Output (10 classes)
```

### 2. Táº¡i sao cáº§n tf.Graph?

**Lá»£i Ã­ch:**
1. **Tá»‘i Æ°u hÃ³a:** TensorFlow cÃ³ thá»ƒ tá»‘i Æ°u toÃ¡n Ä‘á»“ thá»‹ trÆ°á»›c khi cháº¡y (fusion, pruning)
2. **Parallel execution:** CÃ¡c node Ä‘á»™c láº­p cháº¡y song song
3. **Deployment:** Export graph Ä‘á»ƒ deploy lÃªn mobile/server
4. **Performance:** Eager execution (TF 2.x) vs Graph mode (TF 1.x)

### 3. Eager Execution vs Graph Mode

#### Eager Execution (TF 2.x - Default)
```python
# Code cháº¡y ngay láº­p tá»©c, giá»‘ng Python thÃ´ng thÆ°á»ng
import tensorflow as tf

x = tf.constant([[1.0, 2.0]])
y = tf.constant([[3.0], [4.0]])
result = tf.matmul(x, y)
print(result.numpy())  # [[11.]]
```

#### Graph Mode (DÃ¹ng @tf.function)
```python
@tf.function  # Decorator nÃ y compile thÃ nh graph
def my_function(x, y):
    return tf.matmul(x, y)

x = tf.constant([[1.0, 2.0]])
y = tf.constant([[3.0], [4.0]])
result = my_function(x, y)
print(result.numpy())  # [[11.]] - Nhanh hÆ¡n eager mode
```

### 4. Visualize Graph vá»›i TensorBoard

```python
# Táº¡o má»™t model Ä‘Æ¡n giáº£n
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Táº¡o dummy data
import numpy as np
dummy_x = np.random.random((1, 784))
dummy_y = np.array([5])

# Trace graph báº±ng tf.function
@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = model(x, training=True)
        loss = tf.keras.losses.sparse_categorical_crossentropy(y, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    return loss

# Log graph vÃ o TensorBoard
log_dir = "logs/graph"
writer = tf.summary.create_file_writer(log_dir)

# Trace vÃ  log
tf.summary.trace_on(graph=True, profiler=True)
train_step(dummy_x, dummy_y)
with writer.as_default():
    tf.summary.trace_export(name="model_trace", step=0)
```

### 5. Khi nÃ o dÃ¹ng @tf.function?

**NÃªn dÃ¹ng khi:**
- Training loop láº·p Ä‘i láº·p láº¡i nhiá»u láº§n
- Deploy model lÃªn production (tÄƒng tá»‘c 10-50x)
- Model phá»©c táº¡p, cáº§n tá»‘i Æ°u performance

**KhÃ´ng nÃªn dÃ¹ng khi:**
- Debugging (khÃ³ debug hÆ¡n eager mode)
- Prototype nhanh
- Logic phá»©c táº¡p vá»›i Python conditionals

---

## III.D. Max Pooling - Giáº£m kÃ­ch thÆ°á»›c dá»¯ liá»‡u

### 1. Max Pooling lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:** 
Max Pooling lÃ  ká»¹ thuáº­t **giáº£m kÃ­ch thÆ°á»›c** (downsampling) cá»§a feature map báº±ng cÃ¡ch chá»n **giÃ¡ trá»‹ lá»›n nháº¥t** trong má»™t vÃ¹ng nhá».

**Minh há»a trá»±c quan:**

```
Input (4Ã—4):                  MaxPool 2Ã—2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  3  2  4 â”‚              â”‚ 3  4   â”‚
â”‚ 5  6  7  8 â”‚    â”€â”€â†’       â”‚ 9  11  â”‚
â”‚ 3  2  1  0 â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 9  7  5  11â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CÃ¡ch hoáº¡t Ä‘á»™ng:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ 1 â”‚ 3 â”‚ 2 â”‚ 4 â”‚  â†’ Max(1,3,5,6) = 6  â†’ 3
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     Max(2,4,7,8) = 8  â†’ 4
â”‚ 5 â”‚ 6 â”‚ 7 â”‚ 8 â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚ 3 â”‚ 2 â”‚ 1 â”‚ 0 â”‚  â†’ Max(3,2,9,7) = 9  â†’ 9
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     Max(1,0,5,11) = 11 â†’ 11
â”‚ 9 â”‚ 7 â”‚ 5 â”‚11â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
```

### 2. Táº¡i sao cáº§n Max Pooling?

**Lá»£i Ã­ch:**
1. **Giáº£m sá»‘ lÆ°á»£ng parameters:** Ãt neurons hÆ¡n â†’ Model nháº¹ hÆ¡n
2. **Giáº£m overfitting:** Bá» Ä‘i thÃ´ng tin chi tiáº¿t khÃ´ng quan trá»ng
3. **Translation invariance:** Nháº­n diá»‡n váº­t thá»ƒ dÃ¹ nÃ³ dá»‹ch chuyá»ƒn vá»‹ trÃ­
4. **TÄƒng receptive field:** Má»—i neuron "nhÃ¬n" vÃ¹ng lá»›n hÆ¡n cá»§a áº£nh gá»‘c

**VÃ­ dá»¥ thá»±c táº¿:**
```
áº¢nh 28Ã—28 â†’ Conv(32 filters) â†’ 28Ã—28Ã—32 (cÃ²n lá»›n)
          â†“
MaxPool 2Ã—2 â†’ 14Ã—14Ã—32 (giáº£m 75% kÃ­ch thÆ°á»›c)
          â†“
Conv(64 filters) â†’ 14Ã—14Ã—64
          â†“
MaxPool 2Ã—2 â†’ 7Ã—7Ã—64 (giáº£m tiáº¿p 75%)
          â†“
Flatten â†’ 3,136 neurons (thay vÃ¬ 25,088)
```

### 3. Code sá»­ dá»¥ng Max Pooling

```python
model = tf.keras.models.Sequential([
    # Conv Layer 1
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', 
                          input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(2, 2),  # 28Ã—28 â†’ 14Ã—14
    
    # Conv Layer 2
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),  # 14Ã—14 â†’ 7Ã—7
    
    # Fully Connected
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.summary()
```

Output:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320       
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         
conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     
max_pooling2d_1 (MaxPooling2D)(None, 5, 5, 64)         0         
flatten (Flatten)            (None, 1600)              0         
dense (Dense)                (None, 128)               204928    
dense_1 (Dense)              (None, 10)                1290      
=================================================================
Total params: 225,034
```

### 4. So sÃ¡nh cÃ¡c loáº¡i Pooling

| Loáº¡i | CÃ´ng thá»©c | Khi nÃ o dÃ¹ng |
|------|-----------|--------------|
| **Max Pooling** | $\max(x_{ij})$ | Nháº­n diá»‡n features ná»•i báº­t (edges, textures) |
| **Average Pooling** | $\frac{1}{n}\sum x_{ij}$ | LÃ m mÆ°á»£t, giáº£m noise |
| **Global Average Pooling** | Average toÃ n bá»™ feature map | Thay tháº¿ Flatten, giáº£m overfitting |

```python
# Max Pooling
tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)

# Average Pooling
tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2)

# Global Average Pooling (giáº£m 7Ã—7Ã—64 â†’ 1Ã—1Ã—64)
tf.keras.layers.GlobalAveragePooling2D()
```

### 5. Visualize hiá»‡u á»©ng Max Pooling

```python
import matplotlib.pyplot as plt
import numpy as np

# Táº¡o áº£nh test
img = training_images[0].reshape(28, 28)

# Táº¡o model chá»‰ cÃ³ Conv + MaxPool
test_model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', 
                          input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(2, 2)
])

# Xem káº¿t quáº£
result = test_model.predict(img.reshape(1, 28, 28, 1))

# Plot
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(img, cmap='gray')
axes[0].set_title('Original (28Ã—28)')

axes[1].imshow(result[0, :, :, 0], cmap='gray')
axes[1].set_title('After Conv (26Ã—26)')

axes[2].imshow(result[0, :, :, 0], cmap='gray')
axes[2].set_title('After MaxPool (13Ã—13)')
plt.show()
```

---

## III.E. Black Box trong Deep Learning

### 1. Black Box lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:**
"Black Box" lÃ  hiá»‡n tÆ°á»£ng mÃ  ta **biáº¿t input vÃ  output**, nhÆ°ng **khÃ´ng hiá»ƒu cÃ¡ch mÃ´ hÃ¬nh Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh bÃªn trong**.

**Minh há»a:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ğŸ BLACK BOX              â”‚
â”‚                                  â”‚
â”‚  Input: áº¢nh chÃ³                 â”‚
â”‚     â†“                           â”‚
â”‚  [784 neurons]                  â”‚
â”‚     â†“                           â”‚
â”‚  [128 neurons] â† ??? weights    â”‚
â”‚     â†“                           â”‚
â”‚  [64 neurons]  â† ??? logic      â”‚
â”‚     â†“                           â”‚
â”‚  [10 neurons]                   â”‚
â”‚     â†“                           â”‚
â”‚  Output: 95% cháº¯c lÃ  chÃ³        â”‚
â”‚                                  â”‚
â”‚  â“ Táº¡i sao láº¡i 95%?            â”‚
â”‚  â“ NÃ³ nhÃ¬n vÃ o Ä‘Ã¢u?            â”‚
â”‚  â“ Náº¿u sai, sai á»Ÿ Ä‘Ã¢u?         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Táº¡i sao Deep Learning lÃ  Black Box?

**NguyÃªn nhÃ¢n:**
1. **QuÃ¡ nhiá»u parameters:** Model cÃ³ hÃ ng triá»‡u weights, con ngÆ°á»i khÃ´ng thá»ƒ xem háº¿t
2. **Non-linear transformations:** Nhiá»u lá»›p ReLU, Sigmoid lÃ m relationship phá»©c táº¡p
3. **High-dimensional space:** Dá»¯ liá»‡u Ä‘Æ°á»£c transform qua khÃ´ng gian 128D, 256D...

**VÃ­ dá»¥:**
```python
# Model Fashion-MNIST: 101,770 parameters
# LÃ m sao kiá»ƒm tra 101,770 con sá»‘ nÃ y Ä‘á»ƒ hiá»ƒu logic?
model.get_weights()[0].shape  # (784, 128) = 100,352 weights
```

### 3. Táº¡i sao Black Box láº¡i lÃ  váº¥n Ä‘á»?

**Trong cÃ¡c lÄ©nh vá»±c quan trá»ng:**
- **Y táº¿:** AI cháº©n Ä‘oÃ¡n ung thÆ° â†’ BÃ¡c sÄ© cáº§n biáº¿t "Táº¡i sao AI nghÄ© lÃ  ung thÆ°?"
- **PhÃ¡p lÃ½:** AI tá»« chá»‘i cho vay â†’ NgÆ°á»i dÃ¹ng cÃ³ quyá»n biáº¿t lÃ½ do
- **Tá»± lÃ¡i xe:** AI quyáº¿t Ä‘á»‹nh phanh/ráº½ â†’ Cáº§n giáº£i thÃ­ch cho tai náº¡n

**VÃ­ dá»¥ thá»±c táº¿:** 
Model phÃ¢n loáº¡i áº£nh Ä‘á»™ng váº­t Ä‘áº¡t 98% accuracy, nhÆ°ng khi test:
- Model dá»± Ä‘oÃ¡n "CHÃ“" khi tháº¥y áº£nh cÃ³... **cá» xanh** (vÃ¬ áº£nh chÃ³ trong dataset Ä‘á»u cÃ³ cá»)
- Model khÃ´ng há»c Ä‘áº·c Ä‘iá»ƒm CHÃ“, mÃ  há»c **background pattern**!

### 4. CÃ¡c ká»¹ thuáº­t "má»Ÿ" Black Box (Explainable AI)

#### A. Visualize Intermediate Layers
```python
# Xem layer 1 há»c Ä‘Æ°á»£c gÃ¬
layer_outputs = [layer.output for layer in model.layers[:3]]
activation_model = tf.keras.models.Model(
    inputs=model.input, 
    outputs=layer_outputs
)

# Predict vÃ  xem activation
activations = activation_model.predict(test_images[0].reshape(1, 28, 28, 1))

# Plot
import matplotlib.pyplot as plt
plt.imshow(activations[0][0, :, :, 0], cmap='viridis')
plt.title('Layer 1 - Feature Map 0')
plt.show()
```

#### B. Grad-CAM (Gradient-weighted Class Activation Mapping)
```python
# Hiá»ƒn thá»‹ vÃ¹ng nÃ o model "nhÃ¬n" Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # Táº¡o model output cáº£ predictions vÃ  activations
    grad_model = tf.keras.models.Model(
        [model.inputs], 
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]
    
    # Gradient cá»§a class wrt activations
    grads = tape.gradient(class_channel, last_conv_layer_output)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    
    # Weight activation maps by gradients
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
```

#### C. LIME (Local Interpretable Model-agnostic Explanations)
```python
# Install: pip install lime
from lime import lime_image

explainer = lime_image.LimeImageExplainer()
explanation = explainer.explain_instance(
    test_images[0], 
    model.predict, 
    top_labels=3, 
    hide_color=0, 
    num_samples=1000
)

# Hiá»ƒn thá»‹ vÃ¹ng quan trá»ng nháº¥t
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0], 
    positive_only=True, 
    num_features=5, 
    hide_rest=False
)
plt.imshow(temp)
```

#### D. Feature Importance vá»›i Permutation
```python
# Äo xem feature nÃ o quan trá»ng báº±ng cÃ¡ch shuffle nÃ³
import numpy as np

def feature_importance(model, X_test, y_test):
    baseline_acc = model.evaluate(X_test, y_test, verbose=0)[1]
    importance = []
    
    for i in range(X_test.shape[1]):
        X_permuted = X_test.copy()
        np.random.shuffle(X_permuted[:, i])  # Shuffle feature i
        permuted_acc = model.evaluate(X_permuted, y_test, verbose=0)[1]
        importance.append(baseline_acc - permuted_acc)
    
    return importance
```

### 5. Trade-off: Accuracy vs Interpretability

```
High Interpretability
    â†‘
    â”‚  Linear Regression
    â”‚  Decision Tree (shallow)
    â”‚  
    â”‚  Random Forest
    â”‚  
    â”‚  Neural Network (small)
    â”‚  
    â”‚  Deep Neural Network
    â”‚  ResNet, Transformers
    â†“
Low Interpretability (Black Box)
    
    Low Accuracy â†’ High Accuracy â†’
```

**NguyÃªn táº¯c:**
- **High-stakes decisions:** Æ¯u tiÃªn interpretability (y táº¿, phÃ¡p lÃ½)
- **Low-stakes, high-volume:** Æ¯u tiÃªn accuracy (gá»£i Ã½ phim, quáº£ng cÃ¡o)

---

## III.F. Accuracy vs Validation Accuracy - Hiá»ƒu Ä‘Ãºng Ä‘á»ƒ trÃ¡nh Overfitting

### 1. Äá»‹nh nghÄ©a vÃ  sá»± khÃ¡c biá»‡t

| Metric | Äá»‹nh nghÄ©a | Ã nghÄ©a |
|--------|-----------|---------|
| **Accuracy** | Äá»™ chÃ­nh xÃ¡c trÃªn **training set** | Model há»c tá»‘t dá»¯ liá»‡u huáº¥n luyá»‡n nhÆ° tháº¿ nÃ o |
| **Val_accuracy** | Äá»™ chÃ­nh xÃ¡c trÃªn **validation set** | Model tá»•ng quÃ¡t hÃ³a tá»‘t vá»›i dá»¯ liá»‡u chÆ°a tháº¥y nhÆ° tháº¿ nÃ o |

**Minh há»a:**
```
Dataset (70,000 áº£nh)
    â†“
â”œâ”€ Training Set (60,000) â†’ DÃ¹ng Ä‘á»ƒ há»c
â”‚                        â†’ TÃ­nh ACCURACY
â”‚
â””â”€ Validation Set (10,000) â†’ DÃ¹ng Ä‘á»ƒ kiá»ƒm tra
                           â†’ TÃ­nh VAL_ACCURACY
```

### 2. CÃ¡c trÆ°á»ng há»£p phÃ¢n tÃ­ch

#### Case 1: Healthy Model (MÃ´ hÃ¬nh tá»‘t)
```
Epoch 1: loss=0.5, acc=0.85  | val_loss=0.52, val_acc=0.83
Epoch 2: loss=0.4, acc=0.88  | val_loss=0.43, val_acc=0.86
Epoch 3: loss=0.3, acc=0.91  | val_loss=0.35, val_acc=0.89
Epoch 4: loss=0.25, acc=0.93 | val_loss=0.30, val_acc=0.91
```
**ÄÃ¡nh giÃ¡:** âœ… Cáº£ 2 Ä‘á»u tÄƒng Ä‘á»u â†’ Model há»c tá»‘t vÃ  tá»•ng quÃ¡t hÃ³a tá»‘t

#### Case 2: Overfitting (Há»c váº¹t)
```
Epoch 1: loss=0.5, acc=0.85  | val_loss=0.52, val_acc=0.83
Epoch 2: loss=0.3, acc=0.92  | val_loss=0.45, val_acc=0.87
Epoch 3: loss=0.15, acc=0.96 | val_loss=0.55, val_acc=0.85 âš ï¸
Epoch 4: loss=0.08, acc=0.98 | val_loss=0.70, val_acc=0.82 âš ï¸
```
**ÄÃ¡nh giÃ¡:** âŒ Accuracy tÄƒng, Val_accuracy giáº£m â†’ Model há»c thuá»™c lÃ²ng training data

**Minh há»a trá»±c quan:**
```
Accuracy vs Val_Accuracy

Acc â”
    â”‚     â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training (tiáº¿p tá»¥c tÄƒng)
100%â”‚   â•±
    â”‚  â•±
 90%â”‚ â•±    â•±â”€â”€â”€â•²  Validation (tÄƒng rá»“i giáº£m)
    â”‚â•±   â•±      â•²___
 80%â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Epochs
    0   5   10  15  20
        â†‘
    Sweet spot (dá»«ng á»Ÿ Ä‘Ã¢y!)
```

#### Case 3: Underfitting (ChÆ°a há»c Ä‘á»§)
```
Epoch 1: loss=0.8, acc=0.60  | val_loss=0.82, val_acc=0.58
Epoch 2: loss=0.75, acc=0.62 | val_loss=0.78, val_acc=0.60
Epoch 3: loss=0.72, acc=0.64 | val_loss=0.75, val_acc=0.62
```
**ÄÃ¡nh giÃ¡:** âš ï¸ Cáº£ 2 Ä‘á»u tháº¥p vÃ  tÄƒng cháº­m â†’ Model quÃ¡ Ä‘Æ¡n giáº£n

### 3. Code Ä‘á»ƒ monitor vÃ  visualize

```python
import matplotlib.pyplot as plt

# Train model vÃ  lÆ°u history
history = model.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=20,
    verbose=1
)

# Plot Accuracy vs Val_Accuracy
plt.figure(figsize=(12, 4))

# Plot 1: Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.grid(True)

# Plot 2: Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.grid(True)

plt.tight_layout()
plt.show()

# TÃ¬m epoch tá»‘t nháº¥t
best_epoch = np.argmax(history.history['val_accuracy'])
print(f"Best epoch: {best_epoch + 1}")
print(f"Val_accuracy: {history.history['val_accuracy'][best_epoch]:.4f}")
```

### 4. Giáº£i phÃ¡p khi cÃ³ Overfitting (Accuracy >> Val_Accuracy)

#### Solution 1: Early Stopping
```python
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=5,                   # Dá»«ng náº¿u 5 epochs khÃ´ng cáº£i thiá»‡n
    restore_best_weights=True,
    mode='max'
)
```

#### Solution 2: Dropout (Bá» ngáº«u nhiÃªn neurons)
```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),  # Bá» 30% neurons má»—i iteration
    tf.keras.layers.Dense(10, activation='softmax')
])
```

#### Solution 3: Regularization (L1/L2 - sáº½ giáº£i thÃ­ch chi tiáº¿t sau)
```python
from tensorflow.keras import regularizers

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu',
                         kernel_regularizer=regularizers.l2(0.001)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

#### Solution 4: Data Augmentation (TÄƒng cÆ°á»ng dá»¯ liá»‡u)
```python
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1),
])
```

### 5. Rule of thumb - Quy táº¯c ngÃ³n tay cÃ¡i

| Gap (Accuracy - Val_Accuracy) | TÃ¬nh tráº¡ng | HÃ nh Ä‘á»™ng |
|-------------------------------|-----------|-----------|
| **< 5%** | âœ… Healthy | Tiáº¿p tá»¥c train hoáº·c deploy |
| **5-10%** | âš ï¸ Slight overfitting | ThÃªm Dropout, giáº£m complexity |
| **> 10%** | âŒ Severe overfitting | Cáº§n Regularization, Early stopping, hoáº·c thÃªm data |

**VÃ­ dá»¥:**
```python
# Gap = 2% â†’ OK
Accuracy: 0.92, Val_accuracy: 0.90  âœ…

# Gap = 15% â†’ Overfitting!
Accuracy: 0.95, Val_accuracy: 0.80  âŒ
```

---

## III.G. Edge Detection - Ná»n táº£ng cá»§a Computer Vision

### 1. Edge Detection lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:** 
Edge (cáº¡nh) lÃ  vÃ¹ng cÃ³ **sá»± thay Ä‘á»•i Ä‘á»™t ngá»™t vá» cÆ°á»ng Ä‘á»™ sÃ¡ng** (brightness). Edge detection giÃºp tÃ¬m ra contour (Ä‘Æ°á»ng viá»n) cá»§a váº­t thá»ƒ.

**Táº¡i sao quan trá»ng?**
- CNN há»c Ä‘Æ°á»£c edges á»Ÿ layer Ä‘áº§u tiÃªn
- Edges chá»©a thÃ´ng tin hÃ¬nh dáº¡ng quan trá»ng
- Ná»n táº£ng cho nháº­n diá»‡n váº­t thá»ƒ

**Minh há»a:**
```
áº¢nh gá»‘c:        Edges detected:
â”Œâ”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ   â”‚   â†’    â”‚ â”Œâ”€â”€â” â”‚
â”‚      â”‚        â”‚ â”‚  â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”´â”€â”€â”´â”€â”˜
```

### 2. CÃ¡c thuáº­t toÃ¡n Edge Detection

#### A. Sobel Operator (PhÃ¡t hiá»‡n edges theo hÆ°á»›ng)

**NguyÃªn lÃ½:** Sá»­ dá»¥ng 2 kernels Ä‘á»ƒ tÃ­nh gradient theo chiá»u ngang (Gx) vÃ  dá»c (Gy)

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load áº£nh
image = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)

# Sobel X (vertical edges)
sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)

# Sobel Y (horizontal edges)
sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)

# Combine both
sobel_combined = np.sqrt(sobel_x**2 + sobel_y**2)

# Visualize
fig, axes = plt.subplots(2, 2, figsize=(12, 12))
axes[0, 0].imshow(image, cmap='gray')
axes[0, 0].set_title('Original')

axes[0, 1].imshow(sobel_x, cmap='gray')
axes[0, 1].set_title('Sobel X (Vertical edges)')

axes[1, 0].imshow(sobel_y, cmap='gray')
axes[1, 0].set_title('Sobel Y (Horizontal edges)')

axes[1, 1].imshow(sobel_combined, cmap='gray')
axes[1, 1].set_title('Sobel Combined')
plt.show()
```

**Sobel Kernels (Ma tráº­n):**
```
Gx (Vertical edges):     Gy (Horizontal edges):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -1  0  1â”‚             â”‚  1  2  1â”‚
â”‚ -2  0  2â”‚             â”‚  0  0  0â”‚
â”‚ -1  0  1â”‚             â”‚ -1 -2 -1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### B. Canny Edge Detector (Tá»‘t nháº¥t, phá»• biáº¿n nháº¥t)

**Æ¯u Ä‘iá»ƒm:**
- PhÃ¡t hiá»‡n edges má»ng, rÃµ nÃ©t
- Giáº£m noise tá»‘t
- Káº¿t ná»‘i edges thÃ nh contours hoÃ n chá»‰nh

**5 bÆ°á»›c cá»§a Canny:**
1. **Gaussian Blur:** LÃ m má»‹n áº£nh, giáº£m noise
2. **Gradient Calculation:** TÃ­nh intensity gradient (Sobel)
3. **Non-maximum Suppression:** LÃ m má»ng edges
4. **Double Threshold:** PhÃ¢n loáº¡i strong/weak edges
5. **Edge Tracking:** Ná»‘i weak edges vá»›i strong edges

```python
# Canny Edge Detection
edges_canny = cv2.Canny(image, 
                        threshold1=50,   # Lower threshold
                        threshold2=150)  # Upper threshold

# Visualize
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.imshow(image, cmap='gray')
plt.title('Original')

plt.subplot(1, 2, 2)
plt.imshow(edges_canny, cmap='gray')
plt.title('Canny Edges')
plt.show()
```

**Äiá»u chá»‰nh thresholds:**
```python
# Low threshold â†’ nhiá»u edges (cÃ³ noise)
edges_low = cv2.Canny(image, 30, 100)

# High threshold â†’ Ã­t edges (chá»‰ edges máº¡nh)
edges_high = cv2.Canny(image, 100, 200)

# Medium threshold â†’ balanced (khuyáº¿n nghá»‹)
edges_medium = cv2.Canny(image, 50, 150)
```

#### C. Contour Detection (TÃ¬m Ä‘Æ°á»ng viá»n)

**Contour** lÃ  Ä‘Æ°á»ng cong ná»‘i cÃ¡c Ä‘iá»ƒm liÃªn tá»¥c cÃ³ cÃ¹ng mÃ u/intensity.

```python
# TÃ¬m contours tá»« edges
contours, hierarchy = cv2.findContours(
    edges_canny, 
    cv2.RETR_EXTERNAL,     # Chá»‰ láº¥y contours ngoÃ i
    cv2.CHAIN_APPROX_SIMPLE # NÃ©n contours
)

# Váº½ contours lÃªn áº£nh gá»‘c
image_with_contours = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)
cv2.drawContours(image_with_contours, contours, -1, (0, 255, 0), 2)

# Lá»c contours theo diá»‡n tÃ­ch
large_contours = [c for c in contours if cv2.contourArea(c) > 500]

# PhÃ¢n tÃ­ch contours
for i, contour in enumerate(large_contours):
    # TÃ­nh diá»‡n tÃ­ch
    area = cv2.contourArea(contour)
    
    # TÃ­nh chu vi
    perimeter = cv2.arcLength(contour, True)
    
    # Bounding box
    x, y, w, h = cv2.boundingRect(contour)
    
    print(f"Contour {i}: Area={area}, Perimeter={perimeter}")
    cv2.rectangle(image_with_contours, (x, y), (x+w, y+h), (255, 0, 0), 2)

plt.imshow(image_with_contours)
plt.title(f'Found {len(large_contours)} objects')
plt.show()
```

### 3. So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khi nÃ o dÃ¹ng |
|-------------|---------|------------|--------------|
| **Sobel** | Nhanh, Ä‘Æ¡n giáº£n | Nhiá»u noise, edges dÃ y | Prototype nhanh, detect direction |
| **Canny** | Edges má»ng, chÃ­nh xÃ¡c | Cháº­m hÆ¡n, cáº§n tune params | Production, high quality needed |
| **Contours** | TÃ¬m hÃ¬nh dáº¡ng hoÃ n chá»‰nh | Cáº§n preprocessing tá»‘t | Object detection, counting |

### 4. CNN tá»± há»c Edge Detection

**ThÃº vá»‹:** CNN há»c edges tá»± Ä‘á»™ng á»Ÿ layer Ä‘áº§u!

```python
# Visualize filters cá»§a Conv layer Ä‘áº§u tiÃªn
import matplotlib.pyplot as plt

# Láº¥y weights cá»§a layer Ä‘áº§u
first_layer_weights = model.layers[0].get_weights()[0]

# Plot 32 filters (náº¿u cÃ³ 32 filters)
fig, axes = plt.subplots(4, 8, figsize=(16, 8))
for i, ax in enumerate(axes.flat):
    if i < first_layer_weights.shape[3]:
        # Láº¥y filter thá»© i
        filter_img = first_layer_weights[:, :, 0, i]
        ax.imshow(filter_img, cmap='viridis')
        ax.set_title(f'Filter {i}')
        ax.axis('off')
plt.tight_layout()
plt.show()
```

**Káº¿t quáº£:** Báº¡n sáº½ tháº¥y cÃ¡c filters giá»‘ng nhÆ° Sobel, Canny kernels!

```
Filter 0: Vertical edges
Filter 1: Horizontal edges
Filter 2: Diagonal edges
Filter 3: Corners
...
```

---

## III.H. Zero-Centering vÃ  Data Normalization

### 1. Zero-Centering lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:** 
Zero-centering (mean subtraction) lÃ  ká»¹ thuáº­t **dá»‹ch chuyá»ƒn dá»¯ liá»‡u** sao cho **trung bÃ¬nh = 0**.

**CÃ´ng thá»©c:**
$$X_{centered} = X - \text{mean}(X)$$

**Minh há»a báº±ng White Balance trÃªn mÃ¡y áº£nh:**

```
ChÆ°a white balance:         Sau white balance:
áº¢nh bá»‹ vÃ ng (bias)         áº¢nh cÃ¢n báº±ng mÃ u

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŒ… (vÃ ng)  â”‚   â†’        â”‚ ğŸŒ„ (trung tÃ­nh) â”‚
â”‚ R: 200     â”‚             â”‚ R: 0           â”‚
â”‚ G: 180     â”‚             â”‚ G: -20         â”‚
â”‚ B: 100     â”‚             â”‚ B: -100        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                          â†“
Mean = 160             Mean = 0 (centered!)
```

**Giá»‘ng white balance:**
- **White balance:** Loáº¡i bá» color cast (mÃ u lá»‡ch) báº±ng cÃ¡ch Ä‘iá»u chá»‰nh vá» Ä‘iá»ƒm tráº¯ng chuáº©n
- **Zero-centering:** Loáº¡i bá» bias trong data báº±ng cÃ¡ch dá»‹ch vá» mean = 0

### 2. Táº¡i sao cáº§n Zero-Centering?

#### Lá»£i Ã­ch 1: TÄƒng tá»‘c Gradient Descent
```
KhÃ´ng centered:           CÃ³ centered:
(Weights zigzag)         (Weights Ä‘i tháº³ng)

  wâ‚‚                       wâ‚‚
   â†‘                        â†‘
   â”‚   â•±â•²â•±â•²â•±â•²              â”‚    â•²
   â”‚  â•±      â•²             â”‚     â•²
   â”‚ â•±        â•²            â”‚      â•²
   â”‚â•±__________â•²â†’ wâ‚       â”‚_______â•²â†’ wâ‚
   
Cháº­m, khÃ´ng á»•n Ä‘á»‹nh       Nhanh, á»•n Ä‘á»‹nh
```

#### Lá»£i Ã­ch 2: TrÃ¡nh Exploding/Vanishing Gradients
```python
# VÃ­ dá»¥ vá»›i dá»¯ liá»‡u khÃ´ng centered
X = [100, 200, 300, 400]  # Mean = 250
# Sau nhiá»u layer, activations sáº½ quÃ¡ lá»›n â†’ Exploding!

# Sau zero-centering
X_centered = [-150, -50, 50, 150]  # Mean = 0
# Activations cÃ¢n báº±ng hÆ¡n
```

### 3. CÃ¡c ká»¹ thuáº­t Normalization phá»• biáº¿n

#### A. Min-Max Scaling (Chuáº©n hÃ³a vá» [0, 1])
```python
# CÃ´ng thá»©c
X_normalized = (X - X.min()) / (X.max() - X.min())

# Code
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# VÃ­ dá»¥
X = [10, 20, 30, 40]
# â†’ [0, 0.33, 0.67, 1.0]
```
**Khi nÃ o dÃ¹ng:** Pixel values (áº£nh), Neural Networks

#### B. Standardization (Z-score normalization)
```python
# CÃ´ng thá»©c
X_standardized = (X - mean(X)) / std(X)

# Code
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# VÃ­ dá»¥
X = [10, 20, 30, 40]
Mean = 25, Std = 11.18
# â†’ [-1.34, -0.45, 0.45, 1.34]
# Mean = 0, Std = 1
```
**Khi nÃ o dÃ¹ng:** SVM, Linear Regression, PCA

#### C. Mean Subtraction (Zero-centering thuáº§n tÃºy)
```python
# CÃ´ng thá»©c
X_centered = X - mean(X)

# Code
X_centered = X - np.mean(X, axis=0)

# VÃ­ dá»¥
X = [10, 20, 30, 40]
Mean = 25
# â†’ [-15, -5, 5, 15]
# Mean = 0, nhÆ°ng Std giá»¯ nguyÃªn
```
**Khi nÃ o dÃ¹ng:** CNN (thÆ°á»ng káº¿t há»£p vá»›i chia cho 255)

### 4. Best Practice trong Deep Learning

```python
# CÃ¡ch 1: Min-Max (phá»• biáº¿n cho áº£nh)
training_images = training_images / 255.0
test_images = test_images / 255.0
# Káº¿t quáº£: [0, 1]

# CÃ¡ch 2: Standardization (ImageNet preprocessing)
mean = np.array([0.485, 0.456, 0.406])  # ImageNet mean
std = np.array([0.229, 0.224, 0.225])   # ImageNet std
training_images = (training_images - mean) / std
# Káº¿t quáº£: Mean â‰ˆ 0, Std â‰ˆ 1

# CÃ¡ch 3: Zero-center + Scale
mean = np.mean(training_images, axis=0)
std = np.std(training_images, axis=0)
training_images = (training_images - mean) / std
test_images = (test_images - mean) / std  # DÃ¹ng mean/std tá»« training!
```

**âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG:**
```python
# âŒ SAI: Normalize train vÃ  test riÃªng
train_normalized = (train - train.mean()) / train.std()
test_normalized = (test - test.mean()) / test.std()

# âœ… ÄÃšNG: DÃ¹ng mean/std tá»« training cho test
train_mean = train.mean()
train_std = train.std()
train_normalized = (train - train_mean) / train_std
test_normalized = (test - train_mean) / train_std  # DÃ¹ng train_mean/std!
```

### 5. Visualize hiá»‡u quáº£ Zero-Centering

```python
import numpy as np
import matplotlib.pyplot as plt

# Data khÃ´ng centered
X_original = np.array([100, 150, 200, 250, 300])

# Zero-centered
X_centered = X_original - np.mean(X_original)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

axes[0].hist(X_original, bins=10)
axes[0].axvline(np.mean(X_original), color='r', linestyle='--', 
                label=f'Mean = {np.mean(X_original):.1f}')
axes[0].set_title('Original (Mean â‰  0)')
axes[0].legend()

axes[1].hist(X_centered, bins=10)
axes[1].axvline(np.mean(X_centered), color='r', linestyle='--', 
                label=f'Mean = {np.mean(X_centered):.1f}')
axes[1].set_title('Zero-Centered (Mean = 0)')
axes[1].legend()

plt.tight_layout()
plt.show()
```

---

## III.I. Activation Functions Chi Tiáº¿t

### 1. Activation Function lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:** 
HÃ m kÃ­ch hoáº¡t (activation function) lÃ  hÃ m **phi tuyáº¿n** (non-linear) Ä‘Æ°á»£c Ã¡p dá»¥ng sau má»—i layer Ä‘á»ƒ model cÃ³ thá»ƒ há»c cÃ¡c patterns phá»©c táº¡p.

**Táº¡i sao cáº§n Activation?**
```python
# KhÃ´ng cÃ³ activation (chá»‰ linear)
output = W3 * (W2 * (W1 * X))
       = (W3 * W2 * W1) * X
       = W_combined * X
# â†’ Chá»‰ lÃ  Linear Regression, dÃ¹ cÃ³ 100 layers!

# CÃ³ activation (non-linear)
output = relu(W3 * relu(W2 * relu(W1 * X)))
# â†’ CÃ³ thá»ƒ há»c patterns phá»©c táº¡p!
```

### 2. CÃ¡c loáº¡i Activation Functions

#### A. ReLU (Rectified Linear Unit) - â­ Phá»• biáº¿n nháº¥t

**CÃ´ng thá»©c:**
$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

**Code:**
```python
def relu(x):
    return np.maximum(0, x)

# Hoáº·c trong Keras
tf.keras.layers.Dense(128, activation='relu')
```

**Äá»“ thá»‹:**
```
f(x)
  â”‚     â•±
  â”‚    â•±
  â”‚   â•±
  â”‚  â•±
â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€ x
  â”‚ (x<0 â†’ 0)
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… ÄÆ¡n giáº£n, tÃ­nh toÃ¡n nhanh
- âœ… Giáº£i quyáº¿t vanishing gradient problem
- âœ… Sparse activation (nhiá»u neurons = 0)

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Dying ReLU: Náº¿u x < 0, gradient = 0 â†’ neuron "cháº¿t"
- âŒ KhÃ´ng centered (output luÃ´n >= 0)

**Khi nÃ o dÃ¹ng:** Hidden layers cá»§a CNN, MLP (DEFAULT CHOICE)

#### B. Leaky ReLU - Kháº¯c phá»¥c Dying ReLU

**CÃ´ng thá»©c:**
$$\text{Leaky ReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$$

Vá»›i $\alpha = 0.01$ (thÆ°á»ng dÃ¹ng)

**Code:**
```python
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

# Keras
tf.keras.layers.LeakyReLU(alpha=0.01)
# Hoáº·c
tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.01))
```

**Äá»“ thá»‹:**
```
f(x)
  â”‚     â•±
  â”‚    â•±
  â”‚   â•±
  â”‚  â•±
â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€ x
  â”‚â•± (x<0 â†’ 0.01x)
```

**Khi nÃ o dÃ¹ng:** Khi gáº·p dying ReLU problem

#### C. ELU (Exponential Linear Unit)

**CÃ´ng thá»©c:**
$$\text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \leq 0 \end{cases}$$

**Code:**
```python
tf.keras.layers.Dense(128, activation='elu')
```

**Æ¯u Ä‘iá»ƒm:**
- Mean activation gáº§n 0 (zero-centered)
- Smooth gradient

**NhÆ°á»£c Ä‘iá»ƒm:**
- TÃ­nh $e^x$ cháº­m hÆ¡n ReLU

#### D. Sigmoid - Cho output layer (binary classification)

**CÃ´ng thá»©c:**
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**Äá»“ thá»‹:**
```
f(x)
1 â”¤â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚      â•±
0.5â”‚    â•±
  â”‚  â•±
0 â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
 -âˆ    0    +âˆ
```

**Code:**
```python
tf.keras.layers.Dense(1, activation='sigmoid')
```

**Æ¯u Ä‘iá»ƒm:**
- Output trong khoáº£ng (0, 1) â†’ xÃ¡c suáº¥t
- Smooth, differentiable

**NhÆ°á»£c Ä‘iá»ƒm:**
- âŒ Vanishing gradient (gradient gáº§n 0 á»Ÿ 2 Ä‘áº§u)
- âŒ Not zero-centered
- âŒ Cháº­m

**Khi nÃ o dÃ¹ng:** **Output layer** cá»§a binary classification (0 hoáº·c 1)

#### E. Tanh (Hyperbolic Tangent)

**CÃ´ng thá»©c:**
$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

**Äá»“ thá»‹:**
```
f(x)
1 â”¤â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚      â•±
0 â”¤â”€â”€â”€â”€â•±â”€â”€â”€â”€
  â”‚  â•±
-1â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
```

**Code:**
```python
tf.keras.layers.Dense(128, activation='tanh')
```

**Æ¯u Ä‘iá»ƒm:**
- Zero-centered (output trong [-1, 1])
- Tá»‘t hÆ¡n Sigmoid cho hidden layers

**NhÆ°á»£c Ä‘iá»ƒm:**
- Váº«n cÃ³ vanishing gradient

**Khi nÃ o dÃ¹ng:** RNN, LSTM (Ã­t dÃ¹ng cho CNN)

#### F. Softmax - Cho multi-class classification

**CÃ´ng thá»©c:**
$$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$

**Giáº£i thÃ­ch chi tiáº¿t:**
Biáº¿n Ä‘á»•i vector sá»‘ thÃ nh **phÃ¢n phá»‘i xÃ¡c suáº¥t** (tá»•ng = 1)

**VÃ­ dá»¥ cá»¥ thá»ƒ:**
```python
# Input: Logits tá»« layer cuá»‘i
logits = [2.0, 1.0, 0.1]

# TÃ­nh Softmax thá»§ cÃ´ng
import numpy as np
exp_logits = np.exp(logits)  # [7.39, 2.72, 1.11]
softmax_output = exp_logits / np.sum(exp_logits)
print(softmax_output)
# [0.659, 0.242, 0.099]
# â†’ 65.9% class 0, 24.2% class 1, 9.9% class 2
```

**TÃ­nh cháº¥t:**
1. Output trong (0, 1)
2. Tá»•ng táº¥t cáº£ outputs = 1
3. Class cÃ³ logit cao nháº¥t â†’ xÃ¡c suáº¥t cao nháº¥t

**Code:**
```python
tf.keras.layers.Dense(10, activation='softmax')
```

**Khi nÃ o dÃ¹ng:** **Output layer** cá»§a multi-class classification

**Softmax vs Sigmoid:**
```python
# Binary classification (2 classes)
# CÃ¡ch 1: Sigmoid (1 output neuron)
model.add(Dense(1, activation='sigmoid'))
# Output: [0.8] â†’ 80% class 1, 20% class 0

# CÃ¡ch 2: Softmax (2 output neurons)
model.add(Dense(2, activation='softmax'))
# Output: [0.2, 0.8] â†’ 20% class 0, 80% class 1

# Multi-class (>2 classes) â†’ PHáº¢I dÃ¹ng Softmax
model.add(Dense(10, activation='softmax'))
```

### 3. Báº£ng tá»•ng há»£p Activation Functions

| Activation | Range | Khi nÃ o dÃ¹ng | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|-----------|-------|--------------|---------|------------|
| **ReLU** | [0, âˆ) | Hidden layers (DEFAULT) | Nhanh, Ä‘Æ¡n giáº£n | Dying ReLU |
| **Leaky ReLU** | (-âˆ, âˆ) | Hidden layers (náº¿u ReLU cháº¿t) | Fix dying ReLU | ThÃªm hyperparameter Î± |
| **ELU** | (-Î±, âˆ) | Hidden layers (cáº§n performance tá»‘t) | Zero-centered | Cháº­m hÆ¡n ReLU |
| **Sigmoid** | (0, 1) | Binary classification output | XÃ¡c suáº¥t | Vanishing gradient |
| **Tanh** | (-1, 1) | RNN, LSTM | Zero-centered | Vanishing gradient |
| **Softmax** | (0, 1), sum=1 | Multi-class output | XÃ¡c suáº¥t chuáº©n | Chá»‰ dÃ¹ng output layer |

### 4. Code so sÃ¡nh Activations

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-5, 5, 100)

# CÃ¡c activation functions
relu = np.maximum(0, x)
leaky_relu = np.where(x > 0, x, 0.01 * x)
sigmoid = 1 / (1 + np.exp(-x))
tanh = np.tanh(x)
elu = np.where(x > 0, x, 1.0 * (np.exp(x) - 1))

# Plot
plt.figure(figsize=(15, 4))

plt.subplot(1, 5, 1)
plt.plot(x, relu)
plt.title('ReLU')
plt.grid(True)

plt.subplot(1, 5, 2)
plt.plot(x, leaky_relu)
plt.title('Leaky ReLU')
plt.grid(True)

plt.subplot(1, 5, 3)
plt.plot(x, sigmoid)
plt.title('Sigmoid')
plt.grid(True)

plt.subplot(1, 5, 4)
plt.plot(x, tanh)
plt.title('Tanh')
plt.grid(True)

plt.subplot(1, 5, 5)
plt.plot(x, elu)
plt.title('ELU')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### 5. Rule of thumb - Chá»n Activation nÃ o?

```
Hidden Layers:
    Start vá»›i ReLU â†’ Náº¿u gáº·p dying ReLU â†’ Thá»­ Leaky ReLU/ELU
    
Output Layer:
    Binary classification â†’ Sigmoid
    Multi-class classification â†’ Softmax
    Regression (dá»± Ä‘oÃ¡n sá»‘) â†’ Linear (khÃ´ng activation)
```

---

## III.J. Regularization - Chá»‘ng Overfitting

### 1. Overfitting lÃ  gÃ¬? (Ã”n láº¡i)

**Váº¥n Ä‘á»:** Model há»c thuá»™c lÃ²ng training data thay vÃ¬ há»c patterns chung

```
Training accuracy: 98% ğŸ“ˆ
Validation accuracy: 75% ğŸ“‰
â†’ Overfitting! Model quÃ¡ phá»©c táº¡p
```

### 2. L1 Regularization (Lasso)

**CÃ´ng thá»©c:**
$$\text{Loss} = \text{Loss}_{\text{original}} + \lambda \sum_{i} |w_i|$$

**CÆ¡ cháº¿:**
- ThÃªm penalty dá»±a trÃªn **giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i** cá»§a weights
- Äáº©y nhiá»u weights vá» **chÃ­nh xÃ¡c báº±ng 0**
- Táº¡o **sparse model** (nhiá»u weights = 0)

**Code:**
```python
from tensorflow.keras import regularizers

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu',
                         kernel_regularizer=regularizers.l1(0.001)),  # Î» = 0.001
    tf.keras.layers.Dense(10, activation='softmax')
])
```

**Khi nÃ o dÃ¹ng:**
- Muá»‘n **feature selection** (loáº¡i bá» features khÃ´ng quan trá»ng)
- Model cÃ³ quÃ¡ nhiá»u features
- Cáº§n model nháº¹ Ä‘á»ƒ deploy

**VÃ­ dá»¥:**
```python
# TrÆ°á»›c L1: weights = [0.5, 0.3, 0.2, 0.1, 0.05]
# Sau L1:  weights = [0.5, 0.3, 0.0, 0.0, 0.0]  â† 3 weights bá»‹ "kill"
```

### 3. L2 Regularization (Ridge) - â­ Phá»• biáº¿n nháº¥t

**CÃ´ng thá»©c:**
$$\text{Loss} = \text{Loss}_{\text{original}} + \lambda \sum_{i} w_i^2$$

**CÆ¡ cháº¿:**
- ThÃªm penalty dá»±a trÃªn **bÃ¬nh phÆ°Æ¡ng** cá»§a weights
- Äáº©y weights vá» **gáº§n 0** (nhÆ°ng khÃ´ng báº±ng 0)
- **Weight decay** - Giáº£m magnitude cá»§a weights

**Code:**
```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu',
                         kernel_regularizer=regularizers.l2(0.01)),  # Î» = 0.01
    tf.keras.layers.Dense(10, activation='softmax')
])
```

**Khi nÃ o dÃ¹ng:**
- **DEFAULT CHOICE** cho regularization
- Model overfitting nháº¹ Ä‘áº¿n trung bÃ¬nh
- Muá»‘n giá»¯ táº¥t cáº£ features nhÆ°ng giáº£m influence

**VÃ­ dá»¥:**
```python
# TrÆ°á»›c L2: weights = [0.5, 0.3, 0.2, 0.1, 0.05]
# Sau L2:  weights = [0.3, 0.2, 0.1, 0.05, 0.02]  â† Táº¥t cáº£ giáº£m, khÃ´ng ai = 0
```

### 4. Elastic Net (L1 + L2)

**CÃ´ng thá»©c:**
$$\text{Loss} = \text{Loss}_{\text{original}} + \lambda_1 \sum_{i} |w_i| + \lambda_2 \sum_{i} w_i^2$$

**Code:**
```python
# Keras khÃ´ng cÃ³ built-in, pháº£i custom
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu',
                         kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)),
    tf.keras.layers.Dense(10, activation='softmax')
])
```

**Khi nÃ o dÃ¹ng:**
- Muá»‘n **cáº£ feature selection vÃ  weight decay**
- Model ráº¥t phá»©c táº¡p

### 5. Dropout - â­ Máº¡nh nháº¥t cho Neural Networks

**CÆ¡ cháº¿:**
Trong má»—i training step, **ngáº«u nhiÃªn táº¯t** (drop) má»™t sá»‘ neurons

**Minh há»a:**
```
Training iteration 1:       Training iteration 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â— â— Ã— â— Ã—  â”‚            â”‚ Ã— â— â— Ã— â—  â”‚
â”‚  â•²â”‚â•² â”‚â•±    â”‚            â”‚  â•²â”‚â•² â”‚â•±    â”‚
â”‚   â— Ã— â—    â”‚            â”‚   Ã— â— â—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(Ã— = dropped)              (KhÃ¡c nhau má»—i iteration)
```

**Code:**
```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),  # Bá» 50% neurons
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),  # Bá» 30% neurons
    tf.keras.layers.Dense(10, activation='softmax')
])
```

**Táº¡i sao hiá»‡u quáº£?**
- Model khÃ´ng thá»ƒ rely on báº¥t ká»³ neuron cá»¥ thá»ƒ nÃ o
- Há»c features robust hÆ¡n
- Giá»‘ng nhÆ° **ensemble** nhiá»u sub-networks

**Khi nÃ o dÃ¹ng:**
- Overfitting nghiÃªm trá»ng
- Fully connected layers (Dense)
- **KHÃ”NG dÃ¹ng** cho Conv layers (dÃ¹ng Batch Normalization thay tháº¿)

**Dropout rate nÃªn chá»n bao nhiÃªu?**
- Small model: 0.2 - 0.3
- Medium model: 0.4 - 0.5
- Large model: 0.5 - 0.7

### 6. So sÃ¡nh Regularization Techniques

| Ká»¹ thuáº­t | CÆ¡ cháº¿ | Khi nÃ o dÃ¹ng | Strength |
|----------|--------|--------------|----------|
| **L1 (Lasso)** | Weights â†’ 0 | Feature selection | â­â­ |
| **L2 (Ridge)** | Weights â†’ small | General purpose (DEFAULT) | â­â­â­ |
| **Elastic Net** | L1 + L2 | Nhiá»u features tÆ°Æ¡ng quan | â­â­ |
| **Dropout** | Randomly drop neurons | Deep networks | â­â­â­â­ |

### 7. Code vÃ­ dá»¥ vá»›i Regularization

```python
# Model khÃ´ng cÃ³ regularization (Baseline)
model_baseline = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Model vá»›i L2 + Dropout
model_regularized = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu',
                         kernel_regularizer=regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(256, activation='relu',
                         kernel_regularizer=regularizers.l2(0.001)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Train cáº£ 2 models
history_baseline = model_baseline.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=20
)

history_regularized = model_regularized.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=20
)

# So sÃ¡nh
print("Baseline - Train acc:", max(history_baseline.history['accuracy']))
print("Baseline - Val acc:", max(history_baseline.history['val_accuracy']))
print("Regularized - Train acc:", max(history_regularized.history['accuracy']))
print("Regularized - Val acc:", max(history_regularized.history['val_accuracy']))

# Káº¿t quáº£ mong Ä‘á»£i:
# Baseline: Train 98%, Val 85% (Overfitting!)
# Regularized: Train 93%, Val 91% (Better generalization!)
```

### 8. Chiáº¿n lÆ°á»£c chá»‘ng Overfitting

**Step-by-step approach:**
```python
1. PhÃ¡t hiá»‡n overfitting:
   if (train_acc - val_acc) > 0.1:
       print("Overfitting detected!")

2. Thá»­ giáº£i phÃ¡p theo thá»© tá»±:
   a. More data (tá»‘t nháº¥t nhÆ°ng tá»‘n kÃ©m)
   b. Data augmentation (xoay, flip, zoom áº£nh)
   c. Dropout (0.3 - 0.5)
   d. L2 regularization (Î» = 0.001 - 0.01)
   e. Early stopping
   f. Giáº£m model complexity (Ã­t layers/neurons hÆ¡n)

3. Monitor val_accuracy:
   - Náº¿u val_acc tÄƒng â†’ Tiáº¿p tá»¥c
   - Náº¿u val_acc khÃ´ng tÄƒng sau 5 epochs â†’ Dá»«ng
```

---

## III.K. Learning Rate vÃ  Khi nÃ o Äiá»u chá»‰nh

### 1. Learning Rate lÃ  gÃ¬?

**Äá»‹nh nghÄ©a:**
Learning rate (LR) lÃ  **bÆ°á»›c nháº£y** khi optimizer cáº­p nháº­t weights.

**CÃ´ng thá»©c cáº­p nháº­t weights:**
$$w_{\text{new}} = w_{\text{old}} - \text{LR} \times \frac{\partial \text{Loss}}{\partial w}$$

**Minh há»a:**
```
Loss landscape:

        Loss
         â†‘
         â”‚     â•±â•²      â† Global minimum
         â”‚    â•±  â•²
         â”‚   â•±    â•²___â•± â† Local minimum
         â”‚  â•±
         â”‚_â•±________________â†’ Weights

LR lá»›n:   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ (Nháº£y xa, cÃ³ thá»ƒ miss minimum)
LR vá»«a:   â”œâ”€â”€â”€â”€â”€â”€â”¤       (CÃ¢n báº±ng)
LR nhá»:   â”œâ”€â”¤            (Nháº£y cháº­m, chÃ­nh xÃ¡c nhÆ°ng lÃ¢u)
```

### 2. TÃ¡c Ä‘á»™ng cá»§a Learning Rate

#### LR quÃ¡ lá»›n (e.g., LR = 0.1)
```
Epoch 1: loss = 2.5
Epoch 2: loss = 3.0  âš ï¸ (tÄƒng!)
Epoch 3: loss = 2.8
Epoch 4: loss = 4.5  âš ï¸
â†’ Model khÃ´ng há»™i tá»¥, loss nháº£y lung tung
```

#### LR quÃ¡ nhá» (e.g., LR = 0.00001)
```
Epoch 1: loss = 2.5
Epoch 2: loss = 2.498
Epoch 3: loss = 2.496
Epoch 4: loss = 2.494
...
Epoch 100: loss = 2.3
â†’ Há»™i tá»¥ quÃ¡ cháº­m, tá»‘n thá»i gian
```

#### LR vá»«a pháº£i (e.g., LR = 0.001)
```
Epoch 1: loss = 2.5
Epoch 2: loss = 1.8
Epoch 3: loss = 1.2
Epoch 4: loss = 0.8
â†’ Há»™i tá»¥ nhanh vÃ  á»•n Ä‘á»‹nh âœ…
```

### 3. Khi nÃ o cáº§n Ä‘iá»u chá»‰nh Learning Rate?

#### TÃ­n hiá»‡u 1: Loss khÃ´ng giáº£m
```python
# Náº¿u tháº¥y:
Epoch 5: loss = 1.5
Epoch 10: loss = 1.48
Epoch 15: loss = 1.47
â†’ LR quÃ¡ nhá»! TÄƒng lÃªn 10x
```

#### TÃ­n hiá»‡u 2: Loss tÄƒng hoáº·c NaN
```python
# Náº¿u tháº¥y:
Epoch 1: loss = 2.5
Epoch 2: loss = nan
â†’ LR quÃ¡ lá»›n! Giáº£m xuá»‘ng 10x
```

#### TÃ­n hiá»‡u 3: Loss giáº£m rá»“i dá»«ng (plateau)
```python
# Náº¿u tháº¥y:
Epoch 10: val_loss = 0.5
Epoch 15: val_loss = 0.48
Epoch 20: val_loss = 0.48
â†’ Giáº£m LR Ä‘á»ƒ fine-tune
```

### 4. Ká»¹ thuáº­t Ä‘iá»u chá»‰nh Learning Rate

#### A. Learning Rate Decay (Giáº£m dáº§n theo epoch)

```python
# Exponential Decay
initial_lr = 0.001
decay_rate = 0.96
decay_steps = 1000

lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=initial_lr,
    decay_steps=decay_steps,
    decay_rate=decay_rate,
    staircase=True
)

optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
```

**Minh há»a:**
```
LR
  â”‚
0.001â”œâ”€â”€â”€â”€â”€â•²___
  â”‚          â•²___
0.0005â”‚              â•²___
  â”‚                    â•²___
0.00025â”‚                    â•²___
  â”‚_________________________â†’ Epochs
  0    10    20    30    40
```

#### B. ReduceLROnPlateau - Callback tá»± Ä‘á»™ng giáº£m LR

```python
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',      # Theo dÃµi val_loss
    factor=0.5,              # Giáº£m LR xuá»‘ng 50% (LR_new = LR_old * 0.5)
    patience=5,              # Chá» 5 epochs khÃ´ng cáº£i thiá»‡n
    min_lr=1e-7,             # LR tháº¥p nháº¥t
    verbose=1
)

model.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=50,
    callbacks=[reduce_lr]
)

# Output:
# Epoch 15: val_loss did not improve, reducing LR to 0.0005
# Epoch 25: val_loss did not improve, reducing LR to 0.00025
```

#### C. Cyclic Learning Rate (CLR)

**Ã tÆ°á»Ÿng:** LR tÄƒng giáº£m theo chu ká»³, giÃºp thoÃ¡t local minima

```python
# Triangular CLR
def triangular_lr(epoch, base_lr=0.001, max_lr=0.01, step_size=10):
    cycle = np.floor(1 + epoch / (2 * step_size))
    x = np.abs(epoch / step_size - 2 * cycle + 1)
    return base_lr + (max_lr - base_lr) * max(0, (1 - x))

lr_callback = tf.keras.callbacks.LearningRateScheduler(triangular_lr)
```

**Minh há»a:**
```
LR
  â”‚   â•±â•²      â•±â•²      â•±â•²
0.01â”œ  â•±  â•²    â•±  â•²    â•±  â•²
  â”‚ â•±    â•²  â•±    â•²  â•±    â•²
0.001â”œâ”€â”€â”€â”€â”€â”€â•²â•±â”€â”€â”€â”€â”€â”€â•²â•±â”€â”€â”€â”€â”€â”€â•²â†’ Epochs
  0   5   10  15  20  25  30
```

#### D. Learning Rate Finder (TÃ¬m LR tá»‘t nháº¥t)

```python
# Ká»¹ thuáº­t tá»« fastai
import numpy as np
import matplotlib.pyplot as plt

def find_lr(model, X_train, y_train, start_lr=1e-7, end_lr=1, epochs=5):
    num_batches = len(X_train) // 32
    lr_mult = (end_lr / start_lr) ** (1 / num_batches)
    
    lrs = []
    losses = []
    lr = start_lr
    
    for epoch in range(epochs):
        for batch in range(num_batches):
            # Train 1 batch
            tf.keras.backend.set_value(model.optimizer.lr, lr)
            # ... train code ...
            
            lrs.append(lr)
            losses.append(loss)
            lr *= lr_mult
            
            if loss > 4 * min(losses):
                break
    
    # Plot
    plt.plot(lrs, losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Finder')
    plt.show()
    
    # Chá»n LR táº¡i Ä‘iá»ƒm loss giáº£m nhanh nháº¥t
    # ThÆ°á»ng lÃ  1/10 cá»§a LR táº¡i loss tháº¥p nháº¥t

# Sá»­ dá»¥ng
find_lr(model, training_images, training_labels)
```

### 5. Best Practices cho Learning Rate

| Optimizer | Default LR | Recommended Range | Notes |
|-----------|-----------|-------------------|-------|
| **SGD** | 0.01 | 0.001 - 0.1 | Cáº§n tune nhiá»u |
| **Adam** | 0.001 | 0.0001 - 0.01 | ThÆ°á»ng khÃ´ng cáº§n tune |
| **RMSprop** | 0.001 | 0.0001 - 0.01 | Tá»‘t cho RNN |
| **Adagrad** | 0.01 | 0.001 - 0.1 | LR tá»± giáº£m theo thá»i gian |

**Quy trÃ¬nh chá»n Learning Rate:**
```python
1. Start vá»›i default:
   optimizer = tf.keras.optimizers.Adam(lr=0.001)

2. Náº¿u loss khÃ´ng giáº£m sau 5 epochs:
   LR *= 10  # TÄƒng lÃªn 0.01

3. Náº¿u loss tÄƒng hoáº·c NaN:
   LR /= 100  # Giáº£m xuá»‘ng 0.00001

4. Náº¿u train OK nhÆ°ng muá»‘n tá»‘t hÆ¡n:
   Use Learning Rate Scheduler (ReduceLROnPlateau)

5. Fine-tuning cuá»‘i:
   LR = 0.0001 (ráº¥t nhá» Ä‘á»ƒ tinh chá»‰nh)
```

### 6. Code vÃ­ dá»¥ hoÃ n chá»‰nh

```python
# Setup model vá»›i LR scheduling
initial_lr = 0.001

# Callback 1: ReduceLROnPlateau
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

# Callback 2: Custom LR logger
class LRLogger(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        lr = self.model.optimizer.lr
        if hasattr(lr, 'numpy'):
            lr = lr.numpy()
        print(f"\nEpoch {epoch+1}: Learning Rate = {lr:.6f}")

lr_logger = LRLogger()

# Compile model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=initial_lr),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Train with LR scheduling
history = model.fit(
    training_images, training_labels,
    validation_data=(test_images, test_labels),
    epochs=50,
    callbacks=[reduce_lr, lr_logger]
)

# Plot LR changes
lrs = [history.history.get('lr', [initial_lr] * 50)]
plt.plot(lrs)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.yscale('log')
plt.show()
```


